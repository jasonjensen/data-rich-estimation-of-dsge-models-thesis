"""
    generate_states(m, Î¸, ğ’, data, Î›, ğ›™, R; burnin=0, backwards_pass=true, speedup = false, Pâ‚€=nothing)

Generate states using a Kalman filter and smoother, with an option for a speedup.

This function implements a Kalman filter and an optional backward smoothing pass (Carter-Kohn style)
to generate states for a given model `m`, parameters `Î¸`, steady-state `ğ’`, data,
and measurement equation matrices `Î›`, `ğ›™`, `R`.

It includes an optional speedup based on Jungbacker and Koopman (2015).

# Arguments
- `m`: The model object.
- `Î¸`: The parameter vector.
- `ğ’`: The steady-state solution.
- `data`: A Workspace containing the observed data (`X`, `nObs`, `nStates`, `nVars`).
- `Î›`: The observation matrix (loading factors).
- `ğ›™`: The autocorrelation matrix for measurement errors.
- `R`: The covariance matrix for measurement errors.
- `burnin=0`: Number of initial periods to discard from the likelihood calculation.
- `backwards_pass=true`: Whether to perform the backward smoothing pass. If `false`, only filtered states are returned.
- `speedup=false`: Whether to use the Jungbacker and Koopman (2015) speedup for the Kalman filter.
- `Pâ‚€=nothing`: Initial covariance matrix for the states. If `nothing`, it's initialized to zeros or computed from the model.

# Returns
- If `backwards_pass == true`:
    - `Sâ‚œ`: A KeyedArray of smoothed states.
    - `log_likelihood`: The log-likelihood of the data given the parameters.
    - `sum_predicted_states`: Sum of predicted states (used for debugging/diagnostics).
- If `backwards_pass == false`:
    - `Sâ‚œ`: A KeyedArray of filtered states.
    - `log_likelihood`: The log-likelihood of the data given the parameters.
"""
function generate_states(m, Î¸, ğ’, data, Î›, ğ›™, R; burnin=0, backwards_pass=true, speedup = false, Pâ‚€=nothing)
   
   @timeit to "preamble" begin
        # countables
        nObs = data.nObs -1
        nStates = data.nStates
        nVars = data.nVars
        
        # nudges to help with invertibility
        nudge   = Symmetric(1e-12*I(nStates*2))
        nudgeáµ¡  = 1e-7*I(nStates)
        nudgeá¶   = 1e-9*I(nStates*2)
        nudgeá´¿  = 1e-9*I(nVars)
        if speedup
            nudgeá´¿ = 1e-9*I(nStates*2)
        end

        # measurement matrix
        M = zeros(nStates*2, nStates*2)
        M[1:nStates,1:nStates] = Matrix(I(nStates))

        # get the transition matrix
        zeros_nStates = zeros(nStates,nStates)
        G_keyed, H_keyed, SS = get_sparse_solution(m, ğ’)

        # raw transition matrices
        G = Matrix(G_keyed)
        H = Matrix(H_keyed)
        
        # padded transition matrices
        GÌƒáµ¡ = G
        GÌƒ = [GÌƒáµ¡          zeros_nStates
            I(nStates)  zeros_nStates]
        GÌƒáµ€ = GÌƒ'  
        HÌƒ = vcat(Matrix(H), zeros(size(H)))

        # shock matrix sandwich
        Q = I(size(HÌƒ,2)) # Q is just the Identity matrix. H depends on Î¸
        HÌƒQHÌƒáµ€ = Symmetric(HÌƒ*Q*HÌƒ')
        Î£áµ¤ = HÌƒQHÌƒáµ€

        # corrected Î›
        Î›Ìƒ  = [Î› -ğ›™*Î›]
        Î›áµ€ = (Î›Ìƒ )'

        # corrected data
        X = Matrix(data.X)'
        XÌƒ = X[:,2:end] - ğ›™*X[:,1:end-1]

        # Jungbacker and Koopman (2008) speedup
        C = I(nStates)
        Aá´¸ = I(nVars)
        Rá´¸ = copy(R)
        
        if speedup
            Râ»Â¹ = pinv(R)
            # Jungbacker and Koopman (2008) speedup
            @timeit to "C" C = inv(Î›áµ€ * Râ»Â¹ * Î›Ìƒ  + nudge*1000)
            @timeit to "Aá´¸" Aá´¸ = C * Î›áµ€ * Râ»Â¹
            # @assert C' â‰ˆ C
        end
        
        XÌƒá´¸ = Aá´¸ * XÌƒ
        
        Aá´¸Î› = Aá´¸ * Î›Ìƒ 
        Aá´¸Î›áµ€ = Aá´¸Î›'
        Rá´¸ = Symmetric(Aá´¸ * R * Aá´¸') + nudgeá´¿
        Qáµ¡ = (M*Î£áµ¤*M')[1:nStates, 1:nStates]

        
        nudgeá¶  = 1e-9*I(size(Rá´¸,1))
        

        # effective number of variables
        nVarsá´¸ = size(XÌƒá´¸,2)

        # loglike baseline
        loglike_baseline = 0
        if speedup
            vSum = 0
            # ZË£ = Î›Ìƒ  * C
            # Î›áµ¥ = ZË£ * inv(ZË£' * Râ»Â¹ * ZË£) * ZË£' *Râ»Â¹
            Î›áµ¥ = Î›Ìƒ  * inv(Î›áµ€ * Râ»Â¹ * Î›Ìƒ ) * Î›áµ€ *Râ»Â¹
            for i = 1:nObs
                v = view(XÌƒ,:,i) - Î›áµ¥*view(XÌƒ,:,i)
                vAdd = v' * Râ»Â¹ * v
                vSum = vSum + vAdd
            end
            loglike_baseline = -0.5*(nVars-(nStates*2)*nObs)*log(2*pi) - 0.5*nObs*log(det(R)/det(Rá´¸)) -0.5*vSum
        end
        
        ## data structures
        @timeit to "prealloc" begin
            # prediction step
            SÌƒâ‚œâ‚Šâ‚â‚â‚œ = repeat([zeros(nStates*2)], nObs) # Predicted states
            PÌƒâ‚œâ‚Šâ‚â‚â‚œ = repeat([Symmetric(zeros( nStates*2, nStates*2))], nObs) 
            Î·â‚œâ‚Šâ‚â‚â‚œ =  repeat([zeros(nStates*2)], nObs) # prediction errors
            # fâ‚œâ‚Šâ‚â‚â‚œ =  repeat([Symmetric(zeros( nStates*2, nStates*2))], nObs) # prediction error variance
            
            # update step
            SÌƒâ‚œâ‚â‚œ = repeat([zeros(nStates*2)], nObs) # Smoothed states (filtered state meands)
            SÌƒâ‚œâ‚â‚œ = deepcopy(SÌƒâ‚œâ‚â‚œ)
            PÌƒâ‚œâ‚â‚œ = repeat([Symmetric(zeros( nStates*2, nStates*2))], nObs) # Smoothed state covariances

            # Kalman gain
            Kâ‚œ = repeat([zeros( size(Aá´¸Î›áµ€))], nObs)
          
            # log likelihood
            LXÌƒá´¸ = zeros(nObs)
            
            # Initial conditions
            SÌƒâ‚€ = repeat(SS, 2)
            if Pâ‚€ === nothing
                PÌƒâ‚€ = zeros(size(GÌƒ))
            else
                PÌƒâ‚€ = Pâ‚€
            end
          
            # workspaces
            K_workspace = Matrix{Float64}(undef, size(Aá´¸Î›áµ€))
        end
    end

    # Forward pass
    @timeit to "forward pass"  @inbounds @simd for t in 1:nObs-1

        # Make predictions
        @timeit to "predict" begin
            if t == 1
                SÌƒâ‚œâ‚Šâ‚â‚â‚œ[t] = GÌƒ * SÌƒâ‚€
                PÌƒâ‚œâ‚Šâ‚â‚â‚œ[t] = Symmetric(GÌƒ*PÌƒâ‚€*GÌƒáµ€) + HÌƒQHÌƒáµ€
            else
                SÌƒâ‚œâ‚Šâ‚â‚â‚œ[t] = GÌƒ * SÌƒâ‚œâ‚â‚œ[t]
                PÌƒâ‚œâ‚Šâ‚â‚â‚œ[t] = Symmetric(GÌƒ * PÌƒâ‚œâ‚â‚œ[t] * GÌƒáµ€) + HÌƒQHÌƒáµ€ #+ nudge    
            end
            Î·â‚œâ‚Šâ‚â‚â‚œ[t] = view(XÌƒá´¸,:,t+1) -  Aá´¸Î› * SÌƒâ‚œâ‚Šâ‚â‚â‚œ[t] # prediction error
            F =  lu(Aá´¸Î› * PÌƒâ‚œâ‚Šâ‚â‚â‚œ[t] * Aá´¸Î›áµ€ + Rá´¸ + nudgeá¶ ) # variance of prediction error
        end
        
        # Calculate Kalman gain
        @timeit to "inversion" begin
            invf = inv(F)
            mul!(K_workspace, PÌƒâ‚œâ‚Šâ‚â‚â‚œ[t], Aá´¸Î›áµ€)
            Kâ‚œ[t] = K_workspace * invf
            # update_kalman_gain!(Kâ‚œ[t], PÌƒâ‚œâ‚Šâ‚â‚â‚œ[t], Aá´¸Î›áµ€, fâ‚œâ‚Šâ‚â‚â‚œ[t], K_workspace)
        end
        
        # Update
        @timeit to "update" begin
            if t < nObs
               SÌƒâ‚œâ‚â‚œ[t+1] = SÌƒâ‚œâ‚Šâ‚â‚â‚œ[t] + Kâ‚œ[t] * Î·â‚œâ‚Šâ‚â‚â‚œ[t]
               PÌƒâ‚œâ‚â‚œ[t+1] = Symmetric(PÌƒâ‚œâ‚Šâ‚â‚â‚œ[t] - Kâ‚œ[t] * Aá´¸Î› * PÌƒâ‚œâ‚Šâ‚â‚â‚œ[t] + nudge)
            end
            
        end

        #calculate loglikelihood
        @timeit to "loglik" begin 
            detF = det(F)
            if detF > 0.0
                LXÌƒá´¸[t] = -(nVarsá´¸/2)*log(2*pi) -0.5*log(detF) - 0.5*((Î·â‚œâ‚Šâ‚â‚â‚œ[t])'*(invf*Î·â‚œâ‚Šâ‚â‚â‚œ[t]));
            else
                LXÌƒá´¸[t] = -Inf;
            end
        end
    end

    if backwards_pass == false
        # return states and baseline loglikelihood
        @timeit to "hcat" Sâ‚œ = KeyedArray(hcat(SÌƒâ‚œâ‚â‚œ...)[1:nStates,:], Variable = G_keyed.Variables, time=1:nObs)
        return (Sâ‚œ, sum(LXÌƒá´¸[burnin+1:end]) + loglike_baseline)
    end
    
    # Backward pass
    SÌƒâ‚œ = repeat([zeros(nStates)], nObs)
    @timeit to "backward pass" @inbounds @simd for t in nObs:-1:1
        if t == nObs
            tempPÌƒâ‚œâ‚â‚œ = Matrix(PÌƒâ‚œâ‚â‚œ[t][1:nStates,1:nStates])
            if isposdef(tempPÌƒâ‚œâ‚â‚œ) == false
                tempPÌƒâ‚œâ‚â‚œ = make_psd(tempPÌƒâ‚œâ‚â‚œ)
            end
            SÌƒâ‚œ[t] = rand(MvNormal(SÌƒâ‚œâ‚â‚œ[t][1:nStates], tempPÌƒâ‚œâ‚â‚œ))
        elseif t>0
            SÌƒâ‚œâ‚â‚œáµ¡ = @view SÌƒâ‚œâ‚â‚œ[t][1:nStates]
            PÌƒâ‚œâ‚â‚œáµ¡ = @view PÌƒâ‚œâ‚â‚œ[t][1:nStates,1:nStates]
        
            _partial = PÌƒâ‚œâ‚â‚œáµ¡ * GÌƒáµ¡' * inv(Symmetric(GÌƒáµ¡ * PÌƒâ‚œâ‚â‚œáµ¡ * GÌƒáµ¡') + Qáµ¡ + nudgeáµ¡) 
            
            SÌƒâ‚œâ‚â‚œâ‚› = SÌƒâ‚œâ‚â‚œáµ¡ +  _partial * (SÌƒâ‚œ[t+1] - GÌƒáµ¡*SÌƒâ‚œâ‚â‚œáµ¡)
            PÌƒâ‚œâ‚â‚œâ‚› = Symmetric(PÌƒâ‚œâ‚â‚œáµ¡ - _partial * GÌƒáµ¡ * PÌƒâ‚œâ‚â‚œáµ¡ + nudgeáµ¡)
            if isposdef(PÌƒâ‚œâ‚â‚œâ‚›) == false
                PÌƒâ‚œâ‚â‚œâ‚› = make_psd(PÌƒâ‚œâ‚â‚œâ‚›)
            end

            # These states are being drawn conditional on error variances informed by Î“
            @timeit to "MvNormal" SÌƒâ‚œ[t] = rand(MvNormal(SÌƒâ‚œâ‚â‚œâ‚›, PÌƒâ‚œâ‚â‚œâ‚›))
        end
    end

    # return states
    Sâ‚œ = copy(SÌƒâ‚œ)
    insert!(Sâ‚œ, 1, zeros(nStates))
    Sâ‚œ[1] = Sâ‚œ[2]

    @timeit to "hcat" Sâ‚œ = KeyedArray(hcat(Sâ‚œ...), Variable = G_keyed.Variables, time=1:nObs+1)
    
    return (Sâ‚œ, sum(LXÌƒá´¸) + loglike_baseline, sum(sum(SÌƒâ‚œâ‚Šâ‚â‚â‚œ)))

end

"""
    update_kalman_gain!(K::SubArray{Float64}, P::SubArray{Float64}, Î›::Adjoint{Float64}, f::Matrix{Float64}, _workspace::Matrix{Float64})
    update_kalman_gain!(K::Matrix{Float64}, P::Symmetric{Float64}, Î›::Adjoint{Float64}, f::Symmetric{Float64}, _workspace::Matrix{Float64})

Update the Kalman gain `K` in-place.

Calculates `K = P * Î›' * inv(f)` using a workspace to avoid allocations.

# Arguments
- `K`: Kalman gain matrix (output).
- `P`: Covariance matrix of the predicted state.
- `Î›`: Transpose of the observation matrix.
- `f`: Covariance matrix of the prediction error.
- `_workspace`: Pre-allocated workspace matrix.
"""
@views function update_kalman_gain!(K::SubArray{Float64}, P::SubArray{Float64}, Î›::Adjoint{Float64}, f::Matrix{Float64}, _workspace::Matrix{Float64})
    mul!(_workspace, P, Î›)
    K[:,:] = _workspace / f
end
@views function update_kalman_gain!(K::Matrix{Float64}, P::Symmetric{Float64}, Î›::Adjoint{Float64}, f::Symmetric{Float64}, _workspace::Matrix{Float64})
    mul!(_workspace, P, Î›)
    K[:,:] = _workspace / f
end


"""
    kalman(m, ğ’, data, Î›, R=I(data.nVars), A=zeros(data.nVars), Q = I(data.nVars); debug=false, core=false, backward=false, burnin=0)

Perform Kalman filtering and smoothing.

Implements the standard Kalman filter and an optional backward smoothing pass.
The state-space model is defined as:
State transition: `Sâ‚œ = G*Sâ‚œâ‚‹â‚ + H*Îµâ‚œ` where `Îµâ‚œ ~ N(0,Q)`
Measurement: `X = Î›*Sâ‚œ + A + Î½â‚œ` where `Î½â‚œ ~ N(0,R)` (Note: `A` is an intercept/offset term in the measurement equation).

# Arguments
- `m`: The model object.
- `ğ’`: The steady-state solution.
- `data`: A Workspace containing the observed data (`X`, `nObs`, `nStates`, `nVars`).
- `Î›`: The observation matrix.
- `R=I(data.nVars)`: Covariance matrix for measurement errors. Defaults to identity.
- `A=zeros(data.nVars)`: Intercept term in the measurement equation. Defaults to zeros.
- `Q=I(data.nVars)`: Covariance matrix for state shocks. Defaults to identity (Note: the docstring for `H` in the model implies `Q` might be implicitly handled by `H`).
- `debug=false`: If true, prints debugging information.
- `core=false`: If true, uses `data.X_core` instead of `data.X`.
- `backward=false`: If true, performs a backward smoothing pass.
- `burnin=0`: Number of initial periods to discard from the likelihood calculation.

# Returns
- `Sâ‚œ`: A KeyedArray of filtered (if `backward=false`) or smoothed (if `backward=true`) states.
- `log_likelihood`: The log-likelihood of the data given the parameters and model.
"""
function kalman(m, ğ’, data, Î›, R=I(data.nVars), A=zeros(data.nVars), Q = I(data.nVars); debug=false, core=false, backward=false, burnin=0)
    """
    state transition equations:
    Sâ‚œ = G*Sâ‚œâ‚‹â‚ + H*Îµâ‚œ      where Îµâ‚œ ~ N(0,I)
    measurement error equation:
    X = Î›*Sâ‚œ + Î½â‚œ           where Î½â‚œ ~ N(0,R)
    """
    G_keyed, H_keyed, SS = get_sparse_solution(m, ğ’)


    nObs = data.nObs
    nStates = data.nStates
    nVars = data.nVars
    
    G = Matrix(G_keyed)
    Gáµ€ = G'
    H = Matrix(H_keyed)
    X = Matrix(data.X)'
    if core == true
        X = Matrix(data.X_core)'
        nVars = size(X,1)
    end

    HÌƒQHÌƒáµ€ = H*Q*H'  
    Î›áµ€ = Î›'

    SÌƒâ‚œâ‚Šâ‚â‚â‚œ = zeros(nObs, nStates) # Predicted states
    PÌƒâ‚œâ‚Šâ‚â‚â‚œ = zeros(nObs, nStates, nStates) # Predicted state covariances
    Î·â‚œâ‚Šâ‚â‚â‚œ = zeros(nObs, nVars) # prediction errors
    fâ‚œâ‚Šâ‚â‚â‚œ =  zeros(nObs, nVars, nVars) #something?
    
    SÌƒâ‚œâ‚â‚œ = zeros(nObs, nStates) # Smoothed states (filtered state meands)
    PÌƒâ‚œâ‚â‚œ = zeros(nObs, nStates, nStates) # Smoothed state covariances  
    PÌƒâ‚€ = zeros(size(G))
    Kâ‚œ = zeros(nObs, nStates, nVars)
    LX = zeros(nObs)
    SÌƒâ‚€ = SS
    
    nudgeá¶  = 1e-12*I(nVars)
    nudgeá´¾ = 1e-12*I(nStates)

    # workspaces:
    K_workspace = Matrix{Float64}(undef, nStates, nVars)

    # Forward pass
    @timeit to "forward pass" for t in 1:nObs-1
        
        # Prepare views
        áµ¥SÌƒâ‚œâ‚â‚œ = @view SÌƒâ‚œâ‚â‚œ[t,:]
        áµ¥PÌƒâ‚œâ‚â‚œ = @view PÌƒâ‚œâ‚â‚œ[t,:,:]
        áµ¥SÌƒâ‚œâ‚Šâ‚â‚â‚œ = @view SÌƒâ‚œâ‚Šâ‚â‚â‚œ[t,:]
        áµ¥PÌƒâ‚œâ‚Šâ‚â‚â‚œ = @view PÌƒâ‚œâ‚Šâ‚â‚â‚œ[t,:,:]
        áµ¥Î·â‚œâ‚Šâ‚â‚â‚œ = @view Î·â‚œâ‚Šâ‚â‚â‚œ[t,:]
        áµ¥fâ‚œâ‚Šâ‚â‚â‚œ = @view fâ‚œâ‚Šâ‚â‚â‚œ[t,:,:]
        áµ¥Kâ‚œ = @view Kâ‚œ[t,:,:]

        # Make predictions
        if t == 1
            # update
            áµ¥SÌƒâ‚œâ‚Šâ‚â‚â‚œ .= G * SÌƒâ‚€
            áµ¥PÌƒâ‚œâ‚Šâ‚â‚â‚œ .= G*PÌƒâ‚€*Gáµ€ + HÌƒQHÌƒáµ€
        else
            # update
            áµ¥SÌƒâ‚œâ‚Šâ‚â‚â‚œ .= G * áµ¥SÌƒâ‚œâ‚â‚œ
            áµ¥PÌƒâ‚œâ‚Šâ‚â‚â‚œ .= G * áµ¥PÌƒâ‚œâ‚â‚œ * Gáµ€ + HÌƒQHÌƒáµ€    
            
        end

        #errors
        áµ¥Î·â‚œâ‚Šâ‚â‚â‚œ .= view(X,:,t+1) - (Î›*A + Î› * áµ¥SÌƒâ‚œâ‚Šâ‚â‚â‚œ)
        áµ¥fâ‚œâ‚Šâ‚â‚â‚œ .= Î› * áµ¥PÌƒâ‚œâ‚Šâ‚â‚â‚œ * Î›áµ€ + R + nudgeá¶       

        # Calculate Kalman gain
        if !isposdef(áµ¥fâ‚œâ‚Šâ‚â‚â‚œ)
            áµ¥fâ‚œâ‚Šâ‚â‚â‚œ .= make_psd(áµ¥fâ‚œâ‚Šâ‚â‚â‚œ)
        end
        @timeit to "inversion" update_kalman_gain!(áµ¥Kâ‚œ, áµ¥PÌƒâ‚œâ‚Šâ‚â‚â‚œ, Î›áµ€, fâ‚œâ‚Šâ‚â‚â‚œ[t,:,:], K_workspace)
        
        LX[t] = -0.5*nVars*log(2*pi) - 0.5*log(det(áµ¥fâ‚œâ‚Šâ‚â‚â‚œ)) - 0.5 * áµ¥Î·â‚œâ‚Šâ‚â‚â‚œ' * inv(copy(áµ¥fâ‚œâ‚Šâ‚â‚â‚œ)) * áµ¥Î·â‚œâ‚Šâ‚â‚â‚œ

        # Update
        if t < nObs 
            â‚™SÌƒâ‚œâ‚â‚œ = @view SÌƒâ‚œâ‚â‚œ[t+1,:]
            â‚™PÌƒâ‚œâ‚â‚œ = @view PÌƒâ‚œâ‚â‚œ[t+1,:,:]

            â‚™SÌƒâ‚œâ‚â‚œ .= áµ¥SÌƒâ‚œâ‚Šâ‚â‚â‚œ +  áµ¥Kâ‚œ * áµ¥Î·â‚œâ‚Šâ‚â‚â‚œ
            â‚™PÌƒâ‚œâ‚â‚œ .=  Symmetric(áµ¥PÌƒâ‚œâ‚Šâ‚â‚â‚œ - áµ¥Kâ‚œ * Î› * áµ¥PÌƒâ‚œâ‚Šâ‚â‚â‚œ + nudgeá´¾) 
        end
    end

    Sâ‚œ = KeyedArray(SÌƒâ‚œâ‚â‚œ', Variable = G_keyed.Variables, time=1:nObs)

    if backward
        Sâ‚œ = zeros(nObs, nStates)
        LX = zeros(nObs)
        loglikS = 0
        @timeit to "backward pass" for t in nObs:-1:1
            áµ¥SÌƒâ‚œ = @view Sâ‚œ[t,:]
            
            if t == nObs
                if isposdef(PÌƒâ‚œâ‚â‚œ[t,1:nStates,1:nStates]) == false
                    PÌƒâ‚œâ‚â‚œ[t,1:nStates,1:nStates] = make_psd(PÌƒâ‚œâ‚â‚œ[t,1:nStates,1:nStates])
                end
                áµ¥SÌƒâ‚œ .= rand(MvNormal(SÌƒâ‚œâ‚â‚œ[t,1:nStates], PÌƒâ‚œâ‚â‚œ[t,1:nStates,1:nStates]))
            elseif t>2
                â‚™SÌƒâ‚œ =  @view Sâ‚œ[t+1, 1:nStates]
                # these means are huge near the start
                SÌƒâ‚œâ‚â‚œáµ¡ = @view SÌƒâ‚œâ‚â‚œ[t, 1:nStates]
                PÌƒâ‚œâ‚â‚œáµ¡ = @view PÌƒâ‚œâ‚â‚œ[t,1:nStates,1:nStates]
                if isposdef(PÌƒâ‚œâ‚â‚œáµ¡) == false
                    PÌƒâ‚œâ‚â‚œáµ¡ = make_psd(PÌƒâ‚œâ‚â‚œáµ¡) + nudgeá´¾
                end
                # TODO: might need to be HQH (non-transpose)
                _partial = PÌƒâ‚œâ‚â‚œáµ¡ * G' * inv(G * PÌƒâ‚œâ‚â‚œáµ¡ * G' + HÌƒQHÌƒáµ€ + nudgeá´¾) 
                
                # divide by norm here to prevent trends/explosive growth.
                SÌƒâ‚œâ‚â‚œâ‚› = SÌƒâ‚œâ‚â‚œáµ¡ +  (_partial/norm(_partial)) * (â‚™SÌƒâ‚œ - G*SÌƒâ‚œâ‚â‚œáµ¡)
                PÌƒâ‚œâ‚â‚œâ‚› = Symmetric(PÌƒâ‚œâ‚â‚œáµ¡ - _partial * G * PÌƒâ‚œâ‚â‚œáµ¡ + nudgeá´¾)
                if isposdef(PÌƒâ‚œâ‚â‚œâ‚›) == false
                    PÌƒâ‚œâ‚â‚œâ‚› = make_psd(PÌƒâ‚œâ‚â‚œâ‚›)
                end
    
                # These states are being drawn conditional on error variances informed by Î“
                # dist=
                áµ¥SÌƒâ‚œ .= rand(MvNormal(SÌƒâ‚œâ‚â‚œâ‚›, PÌƒâ‚œâ‚â‚œâ‚›))
                LX[t] = logpdf(MvNormal(SÌƒâ‚œâ‚â‚œâ‚›, PÌƒâ‚œâ‚â‚œâ‚›), áµ¥SÌƒâ‚œ)
            end
        end
        Sâ‚œ = KeyedArray(Sâ‚œ', Variable = G_keyed.Variables, time=1:nObs)
    end

    return Sâ‚œ, sum(LX[burnin+1:end])

end

"""
    get_sparse_solution(m::MacroModelling.â„³, ğ’)

Get the sparse state-space solution matrices (G, H) and steady-state (SS) from the model.

This function retrieves the solution of the model `m`, sets the state variables to the provided
steady-state `ğ’`, and extracts the state transition matrix `G`, shock impact matrix `H`,
and the steady-state vector `SS`. It also zeros out small values in the solution matrices.

# Arguments
- `m::MacroModelling.â„³`: The model object.
- `ğ’`: The steady-state vector to impose on the model's solution.

# Returns
- `G`: KeyedArray representing the state transition matrix (mapping `VariablesPast` to `Variables`).
- `H`: KeyedArray representing the shock impact matrix.
- `SS`: KeyedArray representing the steady-state vector.
"""
function get_sparse_solution(m::MacroModelling.â„³, ğ’)
    GH = get_solution(m) #TODO: I only need this for the scaffolding, maybe store somewhere and pass along
    GH[2:end, :] = ğ’'
    mask = abs.(GH) .< 1e-14
    GH[mask] .= 0.0
    nStates = length(GH.Variables)
    
    # make it square
    # prev_states will be the columns
    G = zeros(nStates, nStates)
    for (i,var) âˆˆ enumerate(GH.Variables)
        row_name = Symbol("$(var)â‚â‚‹â‚â‚")
        if row_name âˆˆ GH.Steady_state__States__Shocks
            G[:,i] =  GH(row_name,:)'
        end
    end
    
    shock_vars = Vector{Symbol}()
    for var in GH.Steady_state__States__Shocks
        if contains(String(var), "â‚â‚“â‚")
            push!(shock_vars, var)
        end
    end

    G = KeyedArray(G, Variables=GH.Variables, VariablesPast=GH.Variables)

    H = GH(shock_vars, :)'
    
    return G, H, GH(:Steady_state,:)
end

"""
    get_Pâ‚€(m, ğ’)

Compute the initial covariance matrix `PÌƒâ‚€` for the Kalman filter based on the model's
state-space representation, assuming an augmented state vector.

This function calculates the unconditional covariance matrix of an augmented state
vector (typically `[Sâ‚œ', Sâ‚œâ‚‹â‚']'`, where `Sâ‚œ` is the original state vector).
It does this by solving the discrete Lyapunov equation:
`PÌƒâ‚€ = GÌƒ PÌƒâ‚€ GÌƒ' + HÌƒ Q HÌƒ'`
for `PÌƒâ‚€`, where `GÌƒ` is the transition matrix of the augmented state vector and
`HÌƒ Q HÌƒ'` is its innovation covariance. The solution is found by vectorizing the
equation: `vec(PÌƒâ‚€) = inv(I - GÌƒ âŠ— GÌƒ) * vec(HÌƒ Q HÌƒ')`.

This `PÌƒâ‚€` represents the unconditional variance of the augmented state process and is
often used as the initial state covariance matrix when starting the Kalman filter,
assuming the process is stationary and no specific prior for the initial state is available.

# Arguments
- `m::MacroModelling.â„³`: The model object, used to derive the state-space matrices.
- `ğ’`: The steady-state solution of the model, used to obtain `G_keyed` and `H_keyed` via `get_sparse_solution`.

# Returns
- `PÌƒâ‚€::Matrix{Float64}`: The initial covariance matrix for the augmented state vector.
"""
function get_Pâ‚€(m, ğ’)
    G_keyed, H_keyed, SS = get_sparse_solution(m, ğ’)
    nStates = length(SS)
    zeros_nStates = zeros(nStates,nStates)
    GÌƒ = [Matrix(G_keyed)    zeros_nStates
        I(nStates)  zeros_nStates]
    HÌƒ = vcat(Matrix(H_keyed), zeros(size(H_keyed)))
    Q = I(size(HÌƒ,2)) # Q is just the Identity matrix. H depends on Î¸
    HÌƒQHÌƒáµ€ = Symmetric(HÌƒ*Q*HÌƒ')
    PÌƒâ‚€vec = (I(size(GÌƒ,1)^2) - kron(GÌƒ,GÌƒ)) \ reshape(HÌƒQHÌƒáµ€, size(HÌƒ,1)^2,1);
    PÌƒâ‚€ = reshape(PÌƒâ‚€vec, size(GÌƒ))
    return PÌƒâ‚€
end

"""
    get_log_likelihood_Î¸(m::MacroModelling.â„³, fixed_parameters=Vector{Symbol}())

Calculate the sum of log prior probabilities for the parameters of a model.

This function iterates through the parameters of the model `m`. For each parameter
not listed in `fixed_parameters`, it retrieves its prior distribution using
`prior_Î¸(Val(Symbol(m.model_name)), Val(param), value)` and calculates the
log probability density function (logpdf) of the parameter's current value
under this prior. The sum of these logpdf values is returned.

# Arguments
- `m::MacroModelling.â„³`: The model object, containing parameter definitions and current values.
- `fixed_parameters=Vector{Symbol}()`: A vector of symbols representing parameters
  whose prior log-likelihood should not be included in the sum (e.g., if they are fixed
  or their priors are handled elsewhere).

# Returns
- `total_loglik::Float64`: The sum of the log prior probabilities for the non-fixed parameters.
"""
function get_log_likelihood_Î¸(m::MacroModelling.â„³, fixed_parameters=Vector{Symbol}())
    total_loglik = 0
    for (i, param, value) in zip(1:length(m.parameters), m.parameters, m.parameter_values)
        if param âˆ‰ fixed_parameters
            # println(i, param, value)
            dist = prior_Î¸(Val(Symbol(m.model_name)), Val(param), value)
            loglik = logpdf(dist, value)
            total_loglik += loglik
        end
    end
    return total_loglik
end

"""
    get_log_likelihood_X(m::MacroModelling.â„³, X, S, Î›, ğ›™::Diagonal, R::Diagonal, burnin=0, c=1.0)

Calculate the log-likelihood of the observed data `X` given the states `S` and measurement equation parameters.

The measurement equation is assumed to be:
`Xâ‚œ = Î›Sâ‚œ + eâ‚œ`
`eâ‚œ = ğ›™eâ‚œâ‚‹â‚ + Ïµâ‚œ`
where `Ïµâ‚œ ~ NID(0, R)`.

The function calculates the measurement errors `eâ‚œ` and then the structural measurement
innovations `Ïµâ‚œ`. The log-likelihood is computed as the sum of the log probability
density function (logpdf) values of these innovations `Ïµâ‚œ` under a multivariate
normal distribution `MvNormal(zeros(size(X,2)), R ./ c)`. A burn-in period can be
specified to exclude initial observations from the likelihood calculation. The scaling
factor `c` can be used to adjust the variance of the measurement error distribution.

# Arguments
- `m::MacroModelling.â„³`: The model object (currently unused in the function body but part of the signature).
- `X`: A matrix of observed data, where rows are time periods and columns are variables.
- `S`: A matrix or KeyedArray of states, where columns are time periods and rows (or Variables) correspond to model states.
- `Î›`: The observation matrix linking states to observables.
- `ğ›™::Diagonal`: A diagonal matrix representing the autoregressive coefficients for the measurement errors `eâ‚œ`.
- `R::Diagonal`: A diagonal matrix representing the covariance matrix of the structural measurement innovations `Ïµâ‚œ`.
- `burnin=0`: The number of initial observations to exclude from the likelihood calculation.
- `c=1.0`: A scaling factor for the covariance matrix `R` in the likelihood calculation. `R_effective = R / c`.

# Returns
- `loglik::Float64`: The calculated log-likelihood of the data.
"""
function get_log_likelihood_X(m::MacroModelling.â„³, X, S, Î›, ğ›™::Diagonal, R::Diagonal, burnin=0, c=1.0)
    """
    Xâ‚œ = Î›Sâ‚œ + eâ‚œ
    eâ‚œ = ğ›™eâ‚œâ‚‹â‚ + Ïµâ‚œ where Ïµâ‚œ ~ NID(0,R)
    """
    _X = X[burnin+1:end,:]
    _S = Matrix(S)[:, burnin+1:end]
    e = _X' - Î› * _S
    Ïµ = e[:,2:end] - ğ›™*e[:, 1:end-1]

    errors_dist = MvNormal(zeros(size(_X,2)), R ./ c)
    loglik = sum(logpdf(errors_dist, Ïµ))

    return loglik
end